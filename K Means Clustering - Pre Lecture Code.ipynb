{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ====================================================</h2>\n",
    " <h1>CSCI 5622 - Machine Learning</h1> \n",
    "  <h1>K-Means Clustering</h1> \n",
    " \n",
    " <h4>Dr. Osita Onyejekwe</h4>\n",
    " <br>\n",
    " University of Colorado Boulder, Spring 2024\n",
    "<h2>=====================================================</h2>\n",
    "\n",
    "\n",
    "<h2>Outline</h2>\n",
    "\n",
    "<ul>\n",
    "    \n",
    "  <li>What is clustering?</li>\n",
    "  <li>K-Means Clustering:\n",
    "    <ol>\n",
    "   <li> K-Means Clustering: Demo</li>\n",
    "  <li> K-Means Clustering: The Algorithm</li>\n",
    "  <li>Picking K</li>\n",
    "   <li> Shortcomings of K Means</li>\n",
    "    </ol>\n",
    "    </li>\n",
    "  <li> Applications of K Means Clustering (for another lecture) \n",
    "    <ol>\n",
    "        <li> Image Segmentation</li>\n",
    "        <li>Data Pre-processing</li>\n",
    "    </ol>\n",
    "    </li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Clustering Methods</h2>\n",
    "\n",
    "Often times we deal with observations for which there is no target or response variable. So, there is no way to do any predictions (at least not directly), as we don't have a target variable to predict. In these situations, however, what we instead try to do is uncover some structure within the data, and this is where clustering methods enter the picture.\n",
    "\n",
    "\n",
    "Clustering methods fall into the realm of **unsupervised machine-learning techniques** and refer to a set of techniques designed to identify <b> subgroups</b> or <b>clusters</b> within a data set. In other words, the idea is to partition a data set into subgroups in such a way that the members within each group are as similar as possible to one another and quite distinct from the members of other groups.\n",
    "\n",
    "A natural question is what exactly do we mean by <b> similar</b>? The answer depends on the data set we are dealing with as well as the type of method we use. In what follows we will discuss one of the simplest and widely used clustering method, namely K-Means Clustering.\n",
    "\n",
    "<h3>K-Means Clustering: Demo</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs  # artifical data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create some artificial blobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 2 features but in real life, there are WAY more features (dimensional reduction techniques)\n",
    "\n",
    "# code here \n",
    "\n",
    "def create_dataframe(feat_names, n_feat, n_samp, centers, std, random_state=1):\n",
    "    x,y = make_blobs(n_samples = n_samp, centers = centers, cluster_std = std, n_features=n_feat, random_state = random_state, center_box = (0,6))\n",
    "    cancer = pd.DataFrame()\n",
    "    for name, i in zip(feat_names, range(n_feat)):\n",
    "        cancer[name] = x[:,i]\n",
    "        return cancer,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here \n",
    "\n",
    "df,y = create_dataframe(feat_names=['X1','X2'], n_feat = 2, n_samp = 500, std = 0.3,centers=4, random_state =30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given the following unlabeled (no response variable) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.216186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.407564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.199649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.803320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.377644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1\n",
       "0  4.216186\n",
       "1  5.407564\n",
       "2  6.199649\n",
       "3  3.803320\n",
       "4  5.377644"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a toy example with only two features, let's plot a scatterplot and see what this data looks like. Remember that in real life we almost always deal with higher dimensional data sets so visualizing them may not be possible (at least not without some more work!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# looks like there are 3-4 clusters (i know that there are 4)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# code here \u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX1\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Figure 1\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X2'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# looks like there are 3-4 clusters (i know that there are 4)\n",
    "\n",
    "# code here \n",
    "plt.figure(figsize = (12,8))\n",
    "plt.scatter(df['X1'],df['X2'])\n",
    "plt.xlabel('X1\\\\n\\n Figure 1', fontsize=14)\n",
    "plt.ylabel('X2', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Figure 1 above, it is apparent that there are three different subgroups/clusters in our data. K-Means clustering is a simple way to help us identify these five subfroups. In other words, it will be able to tell us which observations fall into which cluster. It also enables us to properly assign any new observations that we may obtain in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Ipython'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation, rc\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Ipython'"
     ]
    }
   ],
   "source": [
    "# code here \n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import animation\n",
    "from matplotlib import animation, rc\n",
    "from Ipython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nplt.figure(figsize=(22,84))\\n\\nm=11\\n for i in range(1,m):\\n    km=KMeans(n_clusters=4, max_iter=2*i-1,init=\\'random\\', n_init=1, random_state=101)\\n    km.fit(df)\\n    labels=km.labels_\\n         centers=km.cluster_centers_\\n    \\n    plt.subplot(m,2,i+1)\\n    \\n    plt.scatter(df[\\'X1\\'],df[\\'X2\\'],c=labels, cmap=\\'coolwarm\\')\\n    plt.scatter(x=centers[:,0],y=centers[:,1],color=\\'r\\', marker=\\'v\\',s=82)\\n    plt.title(\"# of Iterations {}\".format(2*i-1), fontsize=18)\\n    \\nplt.subplot(m,2,1)\\nplt.scatter(df[\\'X1\\'],df[\\'X2\\'],c=y, cmap=\\'coolwarm\\')# \\n    plt.title(\"Original\",fontsize=18)\\n\\nplt.subplot(m,2,m+1)\\nplt.scatter(df[\\'X1\\'],df[\\'X2\\'],c=y, cmap=\\'coolwarm\\')\\nplt.title(\"Original Data\",fontsize=18)\\n    \\n    \\n# plt.show()\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(22,84))\n",
    "\n",
    "m=11\n",
    " for i in range(1,m):\n",
    "    km=KMeans(n_clusters=4, max_iter=2*i-1,init='random', n_init=1, random_state=101)\n",
    "    km.fit(df)\n",
    "    labels=km.labels_\n",
    "         centers=km.cluster_centers_\n",
    "    \n",
    "    plt.subplot(m,2,i+1)\n",
    "    \n",
    "    plt.scatter(df['X1'],df['X2'],c=labels, cmap='coolwarm')\n",
    "    plt.scatter(x=centers[:,0],y=centers[:,1],color='r', marker='v',s=82)\n",
    "    plt.title(\"# of Iterations {}\".format(2*i-1), fontsize=18)\n",
    "    \n",
    "plt.subplot(m,2,1)\n",
    "plt.scatter(df['X1'],df['X2'],c=y, cmap='coolwarm')# \n",
    "    plt.title(\"Original\",fontsize=18)\n",
    "\n",
    "plt.subplot(m,2,m+1)\n",
    "plt.scatter(df['X1'],df['X2'],c=y, cmap='coolwarm')\n",
    "plt.title(\"Original Data\",fontsize=18)\n",
    "    \n",
    "    \n",
    "# plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# same data data but colored according to the appropdiate cluster, but be aware that I would definetly not \n",
    "# know this in real life \n",
    "\n",
    "# code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs k-means clusters where it first runs one iteration and then it runs 2,3,4 all the way to 10 iterations \n",
    "\n",
    "# note that this scatter plot, despire the coloring is exactly the same as the previous scatter plot but once we \n",
    "# run the simulation you will see in real time how k-means clustering re-assigns the cloors in order to come up with \n",
    "# the picture above.\n",
    "\n",
    "%matplotlib ipympl\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "def init():\n",
    "    scat=plt.scatter(df['X1'],df['X2'], c=np.random.randn(0,4,df.shape[0]))\n",
    "    return scat,\n",
    "\n",
    "def run_knn(m):\n",
    "    \n",
    "    def update_plot(i):\n",
    "\n",
    "        if i==0:\n",
    "            labels=np.random.randint(0,4,df.shape[0])\n",
    "            scat=plt.scatter(df['X1'],df['X2'],c=labels,cmap='Spectral')\n",
    "\n",
    "        else:\n",
    "            km=KMeans(n_clusters=4, max_iter=i,init='random', n_init=1, random_state=1)\n",
    "            km.fit(df)\n",
    "            labels=km.labels_\n",
    "            inertia=km.inertia_\n",
    "            centers=km.cluster_centers_\n",
    "            if (i==1):\n",
    "                cent=plt.scatter(x=centers[:,0],y=centers[:,1],color='r', marker='^',s=200)\n",
    "            elif i==m-1:\n",
    "                cent=plt.scatter(x=centers[:,0],y=centers[:,1],color='r', marker='x',lw=5,s=200)\n",
    "            else:\n",
    "                cent=plt.scatter(x=centers[:,0],y=centers[:,1],color='b', marker='v',s=100)\n",
    "\n",
    "\n",
    "            scat=plt.scatter(df['X1'],df['X2'],c=labels,cmap='Spectral')\n",
    "            plt.title(\"Iteration {}\\n Inertia {:.3f}\".format(i,inertia),fontsize=22)\n",
    "\n",
    "\n",
    "        return scat,\n",
    "\n",
    "    def main():\n",
    "\n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "        scat = plt.scatter(df['X1'],df['X2'],s=80)    \n",
    "\n",
    "        anim = FuncAnimation(fig, update_plot,frames=m, interval=1000, blit=True)\n",
    "\n",
    "        return anim\n",
    "    \n",
    "    \n",
    "    return main()\n",
    "    \n",
    "    \n",
    "\n",
    "run_knn(11)\n",
    "# anim.save('knn.gif', writer='PillowWriter')\n",
    "#HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> How does K-Means clustering work?</h2>\n",
    "\n",
    "The K-Means clustring method is quite intuitive and simple. Let $\\{I\\}_i^K$ denote the collection of sets containing the indicies of each of the observations in our data. We require that these sets satisfy the following conditions\n",
    "\n",
    "$$\\bigcup_i I_i=\\{1,\\dots,n\\},\\,\\text{ and } I_i\\cap I_j=\\emptyset \\, \\forall i\\neq j$$\n",
    "\n",
    "We are going to consider a good clustering if the variation within each cluster is as small as it can possibly be.\n",
    "\n",
    "If we denote the within-cluster variation for cluster $I_k$ by $V(I_k)$ then we want would want to solve the following optimization problem $$\\min_{I_k\\\\ k=1,\\dots,K}\\left\\{\\sum_{k=1}^KV(I_k)\\right\\}$$\n",
    "\n",
    "In other words, we would want to find the right index sets which result in the lowest total within-cluster variation.\n",
    "\n",
    "There are many ways to quantify the <b>within-cluster variation</b>, but perhaps the simplest one is to just take the Euclidean distance. In other words, $$V(I_k)=\\frac{1}{|I_k|}\\sum_{i,j\\in I_k}\\big|\\big|\\vec{x}^{(i)}-\\vec{x}^{(j)}\\big|\\big|^2$$\n",
    "\n",
    "where $$||\\vec{x}^{(i)}-\\vec{x}^{(j)}||^2=\\sum_{k=1}^p\\left(x^{(i)}_k-x^{(j)}_k\\right)^2$$\n",
    "\n",
    "\n",
    "In layman's terms, we want to group together observations that have the smallest Euclidean distance from one another. \n",
    "\n",
    "<h3>The Algorithm</h3>\n",
    "\n",
    "Finding a global solution to the optimization problem above is a difficult problem, since there are essentially $K^n$ ways to partition $n$ observations into $K$ clusters. Searching over this solution space is not feasible especially if $K$ and $n$ are large. The K-Means Algorightm finds a local optimum to the optimization problem above, which for many practical purposes proves to be good enough.\n",
    "\n",
    "Below is a description of the algorithm:\n",
    "\n",
    "<b>Step 1.</b>\n",
    "\n",
    "Randomly initiallize cluster assignments. That is, to each observation assign randomly a number between $1$ and $K$.'\n",
    "\n",
    "<b>Step 2.</b>\n",
    "\n",
    "Iterate the following steps until the cluster assignments no longer change:\n",
    "\n",
    "<b>A.</b> For each of the $K$ clusters compute the cluster <b> centroid</b>; that is the means of the observations assigned to each cluster. For example, the centroid for cluster $I_j$ would be \n",
    "\n",
    "$$\\bar{x}^{(j)}=\\frac{1}{|I_j|}\\sum_{\\vec{x}_i\\in I_j}\\vec{x}_i$$\n",
    "\n",
    "<b>B. </b> Assign each observation to the cluster whose Euclidean distance to its centroid is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How to pick K?</h3>\n",
    "\n",
    "One coarse method of picking $K$ is via the <b>elbow</b> trick. Basically we plot the <b> inertia</b> score as a function of $K$ and pick the $K$ that lies at the <b> elbow </b> of the graph.\n",
    "\n",
    "Let's illustrate it with the example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here \n",
    "inertia_score=[]\n",
    "for i in range (2,15):\n",
    "    km = KMeans(n_clusters = 1)\n",
    "    km.fit(df)\n",
    "    inertia_score.append(km.inertia_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codehere\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.set_style('whitegrid')\n",
    "plt.plot(range(2,15), inertia_score, marker = 'o', markerfacecolor='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that until $K=4$ there is a significant drop in the inertia score, but after that it levels off. So, in this case, even if we didn't know to begin with that our data set had four distinct clusters in it, it would have been reasonable to pick $K=4$. Sometimes, this is not as easy to spot and other more sophisticated ways are required to pick the right $K$.\n",
    "\n",
    "Another such method is the <b>silouhete score</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Limitations of K-Means Method</h2>\n",
    "\n",
    "Despite the fact that $K-Means$ is a fast and very useful method to identify subgroups within data, it has its shortcomings.\n",
    "\n",
    "<b> A.</b> You must specify the number of clusters before you run the algorithm. Obviously this is a challenge, since in unsupervised learning we don't know apriori how many or if we have any clusters in our data to begin with.\n",
    "\n",
    "<b> B. </b> It performs poorly when clusters are of varying sizes, shapes, or densities. For example, if one cluster is of spherical shape, and others have a more elongated ellipsoidal shape, K-Means will most likely perform poorly as it simply measures distances between points and is unable to pick up on such trends.\n",
    "\n",
    "<b> C.</b> It performs poorly on observations that lie on the boundary between two clusters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
